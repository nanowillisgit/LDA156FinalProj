% LDA references 
% Make sure dates are formatted YYYY-MM-DD!

@article{Yu2001ADL,
  title={A {Direct LDA Algorithm for High-Dimensional Data - with
Application to Face Recognition}},
  author={H. Yu and J. Yang},
  journaltitle={Pattern Recognition},
  date={2001},
  volume={34},
  pages={2067-2070}
}

@article{fisher,
title={The {Use of Multiple Measurements in Taxonomic Problems}},
author={R.A. Fisher},
journaltitle={Annals of Eugenics},
date={1936},
volume={7},
number={2},
pages={179-188}
}

@article{altman,
title={Financial {Ratios, Discriminant Analysis and the Prediction of Corporate Bankruptcy}},
author = {Edward I. Altman},
journaltitle={The Journal of Finance},
date = {1968-09},
volume = {23},
number = {4},
pages={589-609}
}

@book{bishop2006pattern,
  title={Pattern Recognition and Machine Learning},
  author={Bishop, C. M.},
  date={2006},
  publisher={Springer}
}

@article{kermanyetal2018,
    title={Identifying {Medical Diagnoses and Treatable Diseases by Image-Based Deep Learning}}, 
    volume={172}, DOI={10.1016/j.cell.2018.02.010}, number={5}, 
    journaltitle={Cell}, 
    author={D. S. Kermany and others}, 
    date={2018}, 
    month={2}, 
    pages={1122–1131}
} 

@book{friedman2001elements,
  title={The Elements of Statistical Learning},
  author={Friedman, J. and Hastie, T. and Tibshirani, R.},
  volume={1(10)},
  date={2013},
  publisher={Springer series in statistics New York}
}

@book{james2013introduction,
  title={An Introduction to Statistical Learning},
  author={James, G. and Witten, D. and Hastie, T. and Tibshirani, R.},
  volume={112},
  date={2013},
  publisher={Springer}
}

@book{trefethen1997numerical,
  title={Numerical Linear Algebra},
  author={Trefethen, L. N. and Bau III, D.},
  volume={50},
  date={1997},
  publisher={Siam}
}


@misc{xiao2017/online,
  author       = {{H. Xiao, K. Rasul, and R. Vollgraf}},
  title        = {Fashion-{MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms}},
  date         = {2017-08-28},
  date         = {2017},
  eprintclass  = {cs.LG},
  eprinttype   = {arXiv},
  eprint       = {cs.LG/1708.07747},
}

@misc{FlowersRecognition,
  author = {A. Mamaev}, 
  title = {{Flowers Recognition}},
  howpublished = {\url{https://www.kaggle.com/alxmamaev/flowers-recognition}},
  note = {Accessed: 2020-10-29}
}

@article{rao,
    title={The {Utilization of Multiple Measurements in Problems of Biological Classification}}, 
    volume={10},
    number={2}, 
    journaltitle={Journal of the Royal Statistical Society}, 
    author={C.R. Rao}, 
    date={1948}, 
    pages={159-203}
}

@article{lietal,
title = {Using {Discriminant Analysis for Multi-class Classification: an Experimental Investigation}},
journaltitle= {Knowledge and Information Systems},
volume={10},
pages={453-472},
date={2006-03-24},
author={Tao Li and Shengzhou Zhu and Mitsunori Ogihara}
}
@online{stardata,
title={Star dataset to predict star types},
author={Deepraj Baidya},
url = {https://www.kaggle.com/deepu1109/star-dataset},
date=2019
}

@article{ledoitwolf,
title = {A Well-conditioned Estimator for Large-dimensional Covariance Matrices},
journal = "Journal of Multivariate Analysis",
volume = "88",
number = "2",
pages = "365 - 411",
year = "2004",
issn = "0047-259X",
doi = "https://doi.org/10.1016/S0047-259X(03)00096-4",
author = "Olivier Ledoit and Michael Wolf",
keywords = "Condition number, Covariance matrix estimation, Empirical Bayes, General asymptotics, Shrinkage",
abstract = "Many applied problems require a covariance matrix estimator that is not only invertible, but also well-conditioned (that is, inverting it does not amplify estimation error). For large-dimensional covariance matrices, the usual estimator—the sample covariance matrix—is typically not well-conditioned and may not even be invertible. This paper introduces an estimator that is both well-conditioned and more accurate than the sample covariance matrix asymptotically. This estimator is distribution-free and has a simple explicit formula that is easy to compute and interpret. It is the asymptotically optimal convex linear combination of the sample covariance matrix with the identity matrix. Optimality is meant with respect to a quadratic loss function, asymptotically as the number of observations and the number of variables go to infinity together. Extensive Monte Carlo confirm that the asymptotic results tend to hold well in finite sample."
}

% @online{LDAProjRepo,
% title = {Math {156 LDA Project}},
% author = "Aatmun Baxi and Abhi Vemulapati and Andy Chen and Johnny Mo",
% url = "https://github.com/warewaware/LDA156FinalProj",
% date = "2020-10-29"
% }