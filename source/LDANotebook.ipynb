{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "mDobZ6oLvuF6"
   },
   "source": [
    "LDA to Predict Star Types: Multiclass Classification\n",
    "---\n",
    "In the below cells we will walk through our implementation of Linear Discriminant Analysis on a multiclass classificaion problem. In particular, we will be predicting star types based on attributes of the star such as temperature, luminosity, size, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false,
    "id": "HqTDxIq7WVz0"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "V6VJEu5KX7K6"
   },
   "source": [
    "## Loading data\n",
    "We first load our data and observe its arrangement for preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 392
    },
    "collapsed": false,
    "id": "c7boMdtjXlYh",
    "outputId": "479586f8-768a-4126-d3b6-99199df3d7a6"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data/star_data.csv\")\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "rPMZABjpZzWC"
   },
   "source": [
    "## Data preprocessing\n",
    "\n",
    "As full disclosure, some editing was done on the source csv to begin with, but these edits were limited to arrangement of columns and grouping certain Star Color fields into one group, namely fixing letter cases and grouping unnecessarily pedantic distinctions of color into one group.\n",
    "\n",
    "\n",
    "We immediately notice a few things:\n",
    "\n",
    "1. The 'Star color' attribute is non-numeric\n",
    "2. The 'Spectral Class' attribute is non-numeric\n",
    "\n",
    "To preprocess the data in order to fix the first two observations, we can simply choose a suitable map of the star color and spectral classes to some integers.\n",
    "\n",
    "We use the following scheme for the Star Color:\n",
    "\n",
    "| Star Color         | Mapped Value |\n",
    "|--------------------|--------------|\n",
    "| Red                | -5           |\n",
    "| Orange-Red         | -4           |\n",
    "| Orange             | -3           |\n",
    "| Pale yellow orange | -2           |\n",
    "| Yellow-White       | -1           |\n",
    "| Whitish            | 0            |\n",
    "| White              | 1            |\n",
    "| Blue-White         | 2            |\n",
    "| Blue               | 3            |\n",
    "\n",
    "We make this choices centered around `'White'` being `'1'`, the redder hues being more negative values, with the bluer hues being more positive.\n",
    "\n",
    "We use the following scheme for the Spectral Class:\n",
    " \n",
    " | Spectral Class | Mapped Value |\n",
    "|----------------|--------------|\n",
    "| O              | 5            |\n",
    "| B              | 4            |\n",
    "| A              | 3            |\n",
    "| F              | 2            |\n",
    "| G              | 1            |\n",
    "| K              | 0            |\n",
    "| M              | -1           |\n",
    "\n",
    "The rationale for these choices is again a simple heuristic that the Sun is classified as a a G type star, hence those stars in the G class will be similar to the Sun, with the redder, dimmer K and M star types being represented with the more negative numbers and the brighter, bluer star types O, B, A, and F being more positive numbers.\n",
    "\n",
    "One thing to keep in mind when choosing the values to map the strings to is "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "id": "OpEPVjb-chox"
   },
   "outputs": [],
   "source": [
    "spectra_map = {'O':5, 'B':4, 'A':3, 'F':2, 'G':1, 'K':0, 'M':-1}\n",
    "data.replace(spectra_map,inplace=True)\n",
    "\n",
    "# Reclassify star class 0 -> -1\n",
    "#data.replace(0, 1, inplace=True)\n",
    "\n",
    "color_map = { 'Red':-5, 'Orange-Red':-4, 'Orange':-3, 'Pale yellow orange':-2, \n",
    "             'Yellow-White':-1, 'Whitish':0, 'White':1,\n",
    "             'Blue-White': 2, 'Blue':2}\n",
    "\n",
    "data.replace(color_map, inplace=True)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "KLA6LPYEy_8Z"
   },
   "source": [
    "## Implementing the algorithm\n",
    "\n",
    "We rely on `scikitlearns`'s bult in model to implement.\n",
    "\n",
    "To adjust the training data test size, simply adjust the `train_n` variable to an `int` for actual size of the data, or a `float` between `0` and `1.0` to define a percentage of the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "id": "Dr0DmCm3zPdJ"
   },
   "outputs": [],
   "source": [
    "# Function to split data into train and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Tools to analyze model accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Optional confusion matrix dependency\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "# Training data set size\n",
    "train_n = 130\n",
    "\n",
    "# Define train and test sets\n",
    "A_train, A_test, b_train, b_test = train_test_split(\n",
    "                              data.loc[:,'Temperature (K)':'Spectral Class']\n",
    "                              , data.loc[:,'Star type'], \n",
    "                                   train_size=train_n, random_state=0)\n",
    "\n",
    "# Initialize model\n",
    "model = LDA(solver='lsqr')\n",
    "\n",
    "# Fit training set\n",
    "model.fit(A_train,b_train)\n",
    "\n",
    "# Load predicted classes and analyze performace\n",
    "predicted = model.predict(A_test)\n",
    "#conf_mat = confusion_matrix(b_test,predicted)\n",
    "\n",
    "#print(conf_mat)\n",
    "print(\"Model accuracy:\", accuracy_score(b_test,predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "mgKauSWu9QFV"
   },
   "source": [
    "## Accuracy analysis\n",
    "The above implementation makes use of the `random_state` input in the `train_test_split` function which keeps the split consistent between multiple executions of the code. See the [scikitlearn documentation](https://https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) for more information. \n",
    "\n",
    "As seen, this results in an accuracy of 99.09%. For a more robust picture of the LDA model accuracy, we take the median of the model's error over several random choices of training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 253
    },
    "collapsed": false,
    "id": "x2sV5HOW-Tx5",
    "outputId": "1816b1fb-b4b7-4b04-a7c1-8398d19dbac2"
   },
   "outputs": [],
   "source": [
    "iterations = 100\n",
    "\n",
    "def LDA_error():\n",
    "  # Define train and test sets\n",
    "  A_train, A_test, b_train, b_test = train_test_split(\n",
    "                              data.loc[:,'Temperature (K)':'Spectral Class']\n",
    "                              , data.loc[:,'Star type'], \n",
    "                                   train_size=train_n)\n",
    "  # Initialize model\n",
    "  model = LDA()\n",
    "  # Fit training set\n",
    "  model.fit(A_train,b_train)\n",
    "  # Load accuracy\n",
    "  return accuracy_score(b_test , model.predict(A_test))\n",
    "\n",
    "errors = np.asarray([ LDA_error() for i in range(iterations) ] )\n",
    "\n",
    "print(f\"Median model accuracy over {len(errors)} iterations:\", np.median(errors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "dZJODDWiBzgN"
   },
   "source": [
    "# LDA as a Supervised Dimension Reduction Technique\n",
    "\n",
    "Having demonstrated the utility of LDA as a multiclass classifier model, we turn to another one of LDA's applications: dimension reduction.\n",
    "\n",
    "Having already learned about Principal Component Analysis (PCA), we feel the only further motivation of the utility of LDA as a dimension reduction routine is to compare it PCA.\n",
    "\n",
    "The primary difference between LDA and PCA as dimension reduction routines is that LDA is supervised, where the presence of the data's class labels is used to reduce the dimensionality of the data by using only the most discriminative attributes. It does this finding a projection subspace that maximizes the between-class scatter and minimizes the within-class scatter. Intuitively this can be though of as finding a subspace that maximizes the 'spread' between two different class labels while keeping the elements of one class a 'tight' as possible.\n",
    "\n",
    "The use cases of LDA and PCA are also different. LDA is primarily used to reduce dimension in the case that class separability are desirable. On the other hand, PCA's ability to preserve the variance in data means it should be used when variance in data is desired.\n",
    "\n",
    "The dimension of the reduced data is necessarily bounded above by `number_of_classes - 1`. In the case of our implementation, this means we will reduce the dimension from full square images to a dimension of 1(!). This is perhaps an extreme application, but instructive nevertheless. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "_9FC-u8XuS5M"
   },
   "source": [
    "## LDA vs. PCA\n",
    "\n",
    "Rather than explain the differences between LDA and PCA on a theoretical level, we practically demonstrate their differences by reducing the dimension of the classic wine quality dataset built into scikitlearn.\n",
    "\n",
    "The wine quality dataset is a collection of 178 samples of distinct wines, noting 13(?) different attributes about each wine, that culminate in quality score. The quality scores are grouped into three classes, labelled `0`, `1`, and `2`.\n",
    "\n",
    "In the below cell we run a very simple script of reducing these 13 dimensions to 2, and plotting the results to compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 607
    },
    "collapsed": false,
    "id": "7fOrzvQHvev-",
    "outputId": "192b2dd9-8d57-4bb5-d7a2-d8ae12675b30"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Load and separate data\n",
    "wine_full = load_wine()\n",
    "wine_data = wine_full.data\n",
    "wine_classes = wine_full.target\n",
    "\n",
    "# Reduce dimension with LDA\n",
    "lda_model = LDA(n_components=2)\n",
    "lda_model.fit(wine_data, wine_classes)\n",
    "lda_projected_data = lda_model.transform(wine_data)\n",
    "\n",
    "# Reduce dimension with PCA\n",
    "pca_model = PCA(n_components=2)\n",
    "pca_model.fit(wine_data)\n",
    "pca_projected_data = pca_model.transform(wine_data)\n",
    "\n",
    "lda_classed = [lda_projected_data[wine_classes == i] for i in range(3)]\n",
    "pca_classed = [pca_projected_data[wine_classes == i] for i in range(3)]\n",
    "\n",
    "# Plot our results to compare\n",
    "plt.figure(figsize=(19,10))\n",
    "cols = ['red', 'blue', 'darkorange']\n",
    "plt.subplot(1,2,1)\n",
    "for cls, col in zip(lda_classed, cols):\n",
    "    plt.scatter(cls[:,0],cls[:,1],color=col)\n",
    "\n",
    "plt.title(\"LDA Projection\")\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "for cls, col in zip(pca_classed, cols):\n",
    "    plt.scatter(cls[:,0],cls[:,1],color=col)\n",
    "\n",
    "plt.title(\"PCA Projection\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "eNjB880kwTzE"
   },
   "source": [
    "As one can see, the supervision of the LDA projection subspace gives it a clear advantage if the goal is to maintain separability between classes.\n",
    "\n",
    "The plots may lead one to believe that PCA is a rather poor method to reduce dimensionality, but it should be remembered that PCA's primary goal is not to preserve class separability (it is unsupervised after all). Instead, PCA is superior in the preservation of **variance**.\n",
    "\n",
    "In sum, if one is to think of dimension reduction as a means to reduce dimension while preserving 'fidelity', the usefulness of PCA and LDA will be determined by how one defines 'fidelity'. If data's distinction as being of a particular class is the desired fidelity, then LDA is appropriate, but if the data's uniqueness from the other data (i.e. variance) is a priority, then PCA is appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false,
    "id": "nlg33k5GO7zk"
   },
   "outputs": [],
   "source": [
    "# We will only need these libraries in addition to those\n",
    "### imported at the beginning\n",
    "import os\n",
    "import glob\n",
    "from random import sample\n",
    "\n",
    "# We'll need OpenCV to process the images\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "8oWmWLi3Z_75"
   },
   "source": [
    "Now we import the data and process it into a workable form for the scikitlearn implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false,
    "id": "S3DzzlvHaSHB"
   },
   "outputs": [],
   "source": [
    "train_path_normal = 'data/chest_xray/chest_xray/train/NORMAL/'\n",
    "train_path_pneum = 'data/chest_xray/chest_xray/train/PNEUMONIA/'\n",
    "\n",
    "normal_ims = [ (im, 0) for im in glob.glob(train_path_normal +  '*.jpeg') ]\n",
    "pneum_ims = [ (im, 1) for im in glob.glob(train_path_pneum + '*.jpeg') ]\n",
    "\n",
    "normal_imvecs = []\n",
    "pneum_imvecs = []\n",
    "\n",
    "# Set desired number of images from train set\n",
    "N = 750\n",
    "\n",
    "normal_choices = sample(range(0,len(normal_ims)), N)\n",
    "pneum_choices = sample(range(0,len(pneum_ims)), N)\n",
    "\n",
    "# Set image resize dimensions\n",
    "dim = 220\n",
    "\n",
    "for i in normal_choices:\n",
    "    pix_vec = cv2.imread( normal_ims[i][0])\n",
    "\n",
    "    pix_vec.resize((dim,dim))\n",
    "    pix_vec = np.append(np.reshape(pix_vec, dim*dim),0)\n",
    "\n",
    "    normal_imvecs.append(pix_vec)\n",
    "\n",
    "\n",
    "for i in pneum_choices:\n",
    "    pix_vec = cv2.imread( pneum_ims[i][0])\n",
    "\n",
    "    pix_vec.resize((dim,dim))\n",
    "    pix_vec = np.append(np.reshape(pix_vec, dim*dim),1)\n",
    "\n",
    "    pneum_imvecs.append(pix_vec)\n",
    "\n",
    "immat = np.concatenate( (np.array(normal_imvecs), np.array(pneum_imvecs)) )\n",
    "\n",
    "# Set numpy see for reproducibility\n",
    "np.random.seed(20)\n",
    "np.random.shuffle(immat)\n",
    "\n",
    "data = immat[:,:(dim**2)]\n",
    "classes = immat[:,(dim**2)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "Vn3CE_Uaae4D"
   },
   "source": [
    "Here we actually implement the LDA function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false,
    "id": "pffXwkf_aic0"
   },
   "outputs": [],
   "source": [
    "# n_components is bounded above by 1 as we are doing a binary classification\n",
    "model = LDA(n_components=1)\n",
    "\n",
    "reduced = model.fit_transform(data,classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "esF_iuHma0zn"
   },
   "source": [
    "Now we plot the results of the reduction.\n",
    "\n",
    "If LDA is to succeed in its goal, we should see relatively good separation between the two classes, despite being projected down to such \n",
    "low dimensional space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false,
    "id": "ICGmzEQBa3gt"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "colors = [ 'red', 'blue' ]\n",
    "\n",
    "zeros = reduced[classes == 0]\n",
    "ones = reduced[classes == 1]\n",
    "\n",
    "zeros = np.reshape(zeros, len(zeros))\n",
    "ones = np.reshape(ones, len(ones))\n",
    "\n",
    "plt.scatter(zeros, np.zeros(len(zeros)), color='red')\n",
    "plt.scatter(ones, np.zeros(len(ones)), color='cyan')\n",
    "\n",
    "plt.title(\"Post-reduction train data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "qMloBGkIbMib"
   },
   "source": [
    "# After dimension reduction\n",
    "At this point we have defined a projection subspace to reduce the dimension of the training data. To make this subspace useful, we can use the projection subspace to reduce the dimensionality of the test data, and then we may apply any choice of classification model on the reduced data.\n",
    "\n",
    "The process of the below cell is identical to the above cells, except we apply `model.transform()` on the test data using the model from the previous cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "id": "JbgMQ622b9AA"
   },
   "outputs": [],
   "source": [
    "test_path_normal = 'data/chest_xray/chest_xray/test/NORMAL/'\n",
    "test_path_pneum = 'data/chest_xray/chest_xray/test/PNEUMONIA/'\n",
    "\n",
    "normal_ims_test = [ (im, 0) for im in glob.glob(test_path_normal +  '*.jpeg') ]\n",
    "pneum_ims_test = [ (im, 1) for im in glob.glob(test_path_pneum + '*.jpeg') ]\n",
    "\n",
    "normal_imvecs_test = []\n",
    "pneum_imvecs_test = []\n",
    "\n",
    "# Set desired number of images from train set\n",
    "test_N = 30\n",
    "\n",
    "normal_choices_test = sample(range(0,len(normal_ims_test)), test_N)\n",
    "pneum_choices_test = sample(range(0,len(pneum_ims_test)), test_N)\n",
    "\n",
    "for i in normal_choices_test:\n",
    "    pix_vec = cv2.imread( normal_ims_test[i][0])\n",
    "\n",
    "    pix_vec.resize((dim,dim))\n",
    "    pix_vec = np.append(np.reshape(pix_vec, dim*dim),0)\n",
    "\n",
    "    normal_imvecs_test.append(pix_vec)\n",
    "\n",
    "\n",
    "for i in pneum_choices_test:\n",
    "    pix_vec = cv2.imread( pneum_ims_test[i][0])\n",
    "\n",
    "    pix_vec.resize((dim,dim))\n",
    "    pix_vec = np.append(np.reshape(pix_vec, dim*dim), 1)\n",
    "\n",
    "    pneum_imvecs_test.append(pix_vec)\n",
    "\n",
    "immat_test = np.concatenate( (np.array(normal_imvecs_test), np.array(pneum_imvecs_test)) )\n",
    "np.random.shuffle(immat_test)\n",
    "test_attr = immat_test[:,:(dim**2)]\n",
    "test_classes = immat_test[:,(dim**2)]\n",
    "\n",
    "# Set numpy see for reproducibility\n",
    "np.random.shuffle(immat_test)\n",
    "\n",
    "model.fit(data,classes)\n",
    "\n",
    "test_reduced = model.transform(test_attr)\n",
    "\n",
    "colors = [ 'red', 'blue' ]\n",
    "\n",
    "zeros = test_reduced[test_classes == 0]\n",
    "ones = test_reduced[test_classes == 1]\n",
    "\n",
    "test_zeros = np.reshape(zeros, len(zeros))\n",
    "test_ones = np.reshape(ones, len(ones))\n",
    "\n",
    "plt.scatter(zeros, np.zeros(len(zeros)), color='red')\n",
    "plt.scatter(ones, np.zeros(len(ones)), color='cyan')\n",
    "\n",
    "plt.title(\"Post-reduction test data to train-fitted model\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": null,
   "name": "LDA Math 156 Final Project",
   "provenance": null
  },
  "kernelspec": {
   "argv": [
    "/usr/bin/python3",
    "-m",
    "ipykernel_launcher",
    "-f",
    "{connection_file}"
   ],
   "display_name": "Python 3",
   "env": null,
   "interrupt_mode": "signal",
   "language": "python",
   "metadata": null,
   "name": "python3"
  },
  "name": "LDANotebook.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
